# %%    PCA
#The function returns the projected data, the variance explained, the loadings, as well as sorted eigenvalues and eigenvectors.
def conventionalPCA(data, variance=0, keep_dims=2, method='eig'):  
    #variance is used if a specific threshold is required, while keep_dims only if a specific number of dimensions is required. Variance overrides keep_dims
    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt
    
    feature_mean = np.mean(data, axis = 0) #We use axis = 0 to calculate mean column wise and not row wise. That means we calculate mean for feature, not sample.
    feature_std = np.std(data, axis = 0) #Same for standard deviation
    normalized_X = (data - feature_mean)/feature_std

    if method == 'eig':
        print('Eigendecomposition is applied.')
        covariance = np.cov(normalized_X.T) #np.cov() uses default N-1.
        
        #Instead of np.cov we can write it like so:
        my_covariance = (1/(data.shape[0]-1)) * normalized_X.T.dot(normalized_X) 
        
        #Now lets calculate the eigens of the covariance
        eigenvalues, eigenvectors = np.linalg.eig(my_covariance)
        eigenvalues_sort = eigenvalues[np.argsort(eigenvalues)[::-1]] #Since it sorts in ascending order, we need to reverse it, so we use [::-1]
        eigenvectors_sort = eigenvectors[:, np.argsort(eigenvalues)[::-1]] #Dimensions 20x20. Rows = Features, Columns = PCs
        
        #We keep dimensions based on the free parameter set in the function
        
        total_var = np.sum(eigenvalues)
        var_explained = eigenvalues_sort/total_var
        
        
        #This step ensure that if the user wants a different variance, it will provide those components.
        # For example, if we want to explain 90% of the variance, we need 3 PCs in this specific dataset.
        # This will adapt the dimensions properly instead of having them hardcoded
        
        while float(np.sum(var_explained[:keep_dims])) <= variance:
            keep_dims += 1
        print(f'A variance of {np.sum(var_explained[:keep_dims])*100} can be explained with {keep_dims} PCs')
                
        #keep_eigenvalues = eigenvalues_sort[:keep_dims]
        keep_eigenvectors = eigenvectors_sort[:, :keep_dims] #Dimensions: 20x2. Rows = Features, Columns = PCs
        #print(keep_eigenvectors[:, 0]) #We can check the first eigenvector. The eigenvectors are column wise here
        
        # Now we can also calculate the loadings which will have as rows the principal components and as columns the features
        # We can then use those in a stem plot or heatmap to check the correlation relationship (negative/positive) with each PC.
        loadings = eigenvectors_sort * np.sqrt(eigenvalues_sort) #Rows are PCs and columns are features.
        
        projected_data = np.dot(normalized_X, keep_eigenvectors)

        
        print('Eigenvectors, eigenvalues and loadings returned from the function are sorted based on the descending order of eigenvalues')
        return(projected_data, loadings, var_explained, eigenvalues_sort, eigenvectors_sort)
     

    elif method == 'svd':
        print('Singular value decomposition (SVD) is applied.')
        
        U, S, V = np.linalg.svd(normalized_X)
        
        #Here S is the square root of eigenvalues, so we square the S to get the eigenvalues and then the variance
        total_var = np.sum(S**2)
        var_explained = (S**2)/total_var
        
        
        while float(np.sum(var_explained[:keep_dims])) <= variance:
            keep_dims += 1
        print(f'A variance of {np.sum(var_explained[:keep_dims])*100} can be explained with {keep_dims} PCs')

        keep_eigenvectors = V.T[:, :keep_dims] #The SVD returns singular values (or PCs) in rows so we transpose it
        
        #I noticed that quite a few values of the eigenvectors generated by eigendecomposition and by SVD are the same absolute values but in the SVD were negative when compared.
        #For that reason we get different loadings and then heatmap
        #The same happens with the projected and the reason the scatterplot is mirrored
        loadings = V.T * S 
        
        projected_data = np.dot(normalized_X, keep_eigenvectors)
        
        print('Eigenvectors (transpose(V)), eigenvalues (S**2) and loadings returned from the function are sorted based on the descending order of eigenvalues')
        return(projected_data, loadings, var_explained, S**2, V.T)   
        

    else:
        raise ValueError('Input Error. Method should be "eig" or "svd"')

    
# %%    Plots

def PCAplots(proj, loadings, var_explained, dims=2, pcs=[1,2], plot='scatter'):
    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt
    

    #cumulative_sum = np.cumsum(var_explained) #We can use this to reverse the Scree. I did not implement it here
    scree_ind = np.array(list(range(1,len(var_explained)+1)))
    pcs_fix = [pc-1 for pc in pcs]
    print('By default the scatter uses 2 PCs, the first and second. You can change dims=3 to do a 3D plot.\nYou can also use a list of the dimensions you want to use when calling the function. pcs=[1,5] will plot PC1 and PC2.')
    
    # Skree Plot
    if plot in ['skree', 'all']:    
        #Here i chose to use variance explained since we can see the actual variance each eigenvalue explains
        #We could also use the cumulative summation, which will reverse the plot, converging at 1.
        plt.plot(scree_ind, var_explained, 'bo-', label='Eigenvalues')
        plt.xlabel('Component Number')
        plt.ylabel('Variance Explained')
        plt.title('Scree Plot')
        plt.xticks(scree_ind)
        #plt.axhline(y=my_thresh, linestyle='--', color='r', label='Threshold') #We can use this to add a parrel to x line showing a threshold
        plt.show()
        
    
    # Heatmap
    if plot in ['heatmap', 'all']:
        #The values in each cell of the heatmap indicate the correlation between the feature and the PCs.
        plt.imshow(loadings, cmap= 'coolwarm', aspect = 'auto')
        plt.colorbar()
        plt.xlabel('Features')
        plt.ylabel('Principal Components')
        plt.title('Loadings Heatmap')
        plt.yticks(np.arange(loadings.shape[0]), np.arange(1, loadings.shape[0]+1))
        plt.show()
        

    # Stem plot
    if plot in ['stem', 'all']:
        #Since the stem plot has to be done seperately for each PC, we will use a loop with the dimensions we want to keep
        # So keeping 2 dimensions will result in 2 stem plots, 1 for PC1, and 1 for PC2. 
        for i in pcs_fix:
            plt.stem(scree_ind, loadings[i])
            plt.xticks(scree_ind)
            plt.xlabel('Features')
            plt.ylabel('Correlation value')
            plt.title(f'Stem plot for PC{i+1} using loadings')
            plt.show()

    
    if plot in ['scatter', 'all']:

        if dims == 3:
            from mpl_toolkits.mplot3d import Axes3D
            fig = plt.figure()
            ax = fig.add_subplot(111, projection='3d')
            ax.scatter(proj[:, pcs_fix[0]], proj[:, pcs_fix[1]], proj[:, pcs_fix[2]], c=y, s=8)
            ax.set_xlabel(f'PC{pcs[0]} ({"{:.2f}".format(var_explained[pcs_fix[0]]*100)}%)')
            ax.set_ylabel(f'PC{pcs[1]} ({"{:.2f}".format(var_explained[pcs_fix[1]]*100)}%)')
            ax.set_zlabel(f'PC{pcs[2]} ({"{:.2f}".format(var_explained[pcs_fix[2]]*100)}%)')
            #ax.set_box_aspect([1, 1, 1]) #Visual change
            ax.view_init(elev=20, azim=0) #I think these options looks pretty nice to view
            plt.show()

  
        if dims == 2:
            plt.scatter(proj[:, pcs_fix[0]], proj[:, pcs_fix[1]], c=y, s=8) 
            plt.xlabel(f'PC{pcs[0]} ({"{:.2f}".format(var_explained[pcs_fix[0]]*100)}%)')
            plt.ylabel(f'PC{pcs[1]} ({"{:.2f}".format(var_explained[pcs_fix[1]]*100)}%)') 
            plt.title('Projected Data')
            #plt.gca().invert_yaxis()  #This part is necessary to get the same results as the scatter with eigendecomposition.
            plt.show()
    
    
    print('The plot variable can be "skree", "heatmap", "stem" or "all".\nBy default the function returns only the scatterplot with the first 2 dimensions.')